# coding=utf-8
import numpy as np
from sklearn.base import BaseEstimator


SVM_PARAMS_DICT = {
    'C': 10.,
    'random_state': 777,
    'iters': 10000,
    'batch_size': 100,
    'step': 0.015
}


class MySVM(BaseEstimator):
    def __init__(self, C, random_state, iters, batch_size, step):
        self.C = C
        self.random_state = random_state
        self.iters = iters
        self.batch_size = batch_size
        self.step = step

    # будем пользоватьс€ этой функцией дл€ подсчЄта <w, x>
    def __predict(self, X):
        return np.dot(X, self.w) + self.w0

    # sklearn нужно, чтобы predict возвращал классы, поэтому оборачиваем наш __predict в это
    def predict(self, X):
        res = self.__predict(X)
        res[res > 0] = 1
        res[res < 0] = 0
        return res

    # производна€ регул€ризатора
    def der_reg(self):
        return 1. / self.C * self.w

    # будем считать стохастический градиент не на одном элементе, а сразу на пачке (чтобы было эффективнее)
    def der_loss(self, x, y):
        # x.shape == (batch_size, features)
        # y.shape == (batch_size,)
        # predict(self, x).shape = (batch_size,)

        # считаем производную по каждой координате на каждом объекте
        # TODO
        derLoss = -1.0 if 1-np.dot(y,self.predict(x)) > 0 else 0.0

        # занулить производные там, где отступ > 1
        # TODO
        derLoss = 0.0 if np.dot(y,self.predict(x)) > 1 else derLoss
        
        # дл€ масштаба возвращаем средний градиент по пачке
        # TODO
        return np.array(derLoss).mean

    def fit(self, X_train, y_train):
        # RandomState дл€ воспроизводитмости
        random_gen = np.random.RandomState(self.random_state)
        
        # получаем размерности матрицы
        size, dim = X_train.shape
        
        # случайна€ начальна€ инициализаци€
        self.w = random_gen.rand(dim)
        self.w0 = random_gen.randn()

        for _ in range(self.iters):  
            # берЄм случайный набор элементов
            rand_indices = random_gen.choice(size, self.batch_size)
            x = X_train[rand_indices]
            y = y_train[rand_indices] * 2 - 1 # исходные метки классов это 0/1 а нам надо -1/1

            # считаем производные
            # TODO
            
            # обновл€емс€ по антиградиенту
            # TODO
            self.w = self.w - (x.T * y).T
        # метод fit дл€ sklearn должен возвращать self
        return self


from sklearn.model_selection import train_test_split
from sklearn import datasets

X, y = datasets.make_classification(
    n_samples=10000, n_features=20, 
    n_classes=2, n_informative=20, 
    n_redundant=0,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,
    random_state=42
)
print(X.shape, X.T.shape)


model = MySVM(10, 777, 1000, 10, 0.15)
model.fit(X_train, y_train)
print(model.w, model.w0)


a = np.array([1,1,1,1])
b = np.array([2,2,2,2])
print(a*b)
